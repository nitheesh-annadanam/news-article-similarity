{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W42DaVvlOqmf",
    "outputId": "e99dea11-13eb-47a3-92e5-8d3b6933ca7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nitheesh.annadanam\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nitheesh.annadanam\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nitheesh.annadanam\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "# Using Google news pretrained word to vectorized.\n",
    "#path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "\n",
    "#Download GoogleNews-vectors-negative300.bin from google into the code path\n",
    "google = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary=True)\n",
    "vocab = google.vocab.keys()\n",
    "\n",
    "#Beautiful Soup for scraping content from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# for removing stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N8aVh-faURsN"
   },
   "outputs": [],
   "source": [
    "#Function to scrape contetn from google and clean it up.\n",
    "def cleanup(link):\n",
    "    \n",
    "    # get the link\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    #Get the content using beautiful soup\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Get all the content in p tag alone (the content of the article)\n",
    "      # (This works well in removing ads and other links and more than 95 % of text after this is related to the article )\n",
    "    \n",
    "    whitelist = ['p']\n",
    "    text_elements = [t for t in soup.find_all(text=True) if t.parent.name in whitelist]\n",
    "    te=\" \".join(text_elements)\n",
    "    \n",
    "    #Cleaning the text by removing special characters and mutliple spaces and full stops.\n",
    "    tene=re.sub(\"[^\\\\.{1},a-zA-Z {1}]\",\" \",te)\n",
    "    tene=tene.replace(\"/t\",\"\")\n",
    "    tene=re.sub(\" +\",\" \",tene)\n",
    "    tene=re.sub(\"\\.+\", \".\",tene)\n",
    "    \n",
    "    #removing stop words\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(tene) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    fin =\" \".join(filtered_sentence)\n",
    "\n",
    "    return(fin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0Uhl09lOtUQ",
    "outputId": "aab9a9fe-d53a-49d8-8dc7-f928fbd02fdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitheesh.annadanam\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "index2word_set = set(google.wv.index2word)\n",
    "\n",
    "# Calculating the average of the google vectors for the article.\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IjfFaTAqURu0"
   },
   "outputs": [],
   "source": [
    "# Link 1 and calculating the average of the vectors\n",
    "#link1 = cleanup(\"https://www.ndtv.com/india-news/coronavirus-38-310-fresh-covid-19-cases-in-india-15-lower-than-yesterday-82-67-lakh-total-cases-so-far-1-23-097-deaths-2319732?pfrom=home-coronavirus_outbreak_coverage\")\n",
    "link1 = cleanup(\"https://www.washingtonpost.com/health/2021/05/30/barbershop-coronavirus-vaccines/?itid=hp-top-table-main\")\n",
    "av1=avg_feature_vector(link1,google,300,index2word_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gXjEUn8QOtYW"
   },
   "outputs": [],
   "source": [
    "# Link 2 and calculating the average of the vectors\n",
    "#link2 = cleanup(\"https://www.indiatoday.in/coronavirus-outbreak/story/coronavirus-another-peak-possible-but-all-depends-behaviour-health-ministry-1737666-2020-11-03\")\n",
    "link2 = cleanup(\"https://www.theguardian.com/world/2021/jun/01/uk-records-no-covid-19-deaths-in-a-day-raising-hope-for-unlocking\")\n",
    "\n",
    "av2=avg_feature_vector(link2,google,300,index2word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDCRcE1ATuqu",
    "outputId": "52ea3e12-4ec8-4eb8-d0c4-5953872e6125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles are 85.72  % similar\n"
     ]
    }
   ],
   "source": [
    "#Calculaying cosine similarity score\n",
    "# Calculating cosine similarity of the average vectors is being used as a measure of the similarity of topics because\n",
    "# the google vectors provide an \"understanding\" of the words being used as similar words are placed closer to each other.\n",
    "sim = 1 - spatial.distance.cosine(av1, av2)\n",
    "print(\"The articles are\",round(sim*100,2),\" % similar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLSh7e-UTuoQ"
   },
   "outputs": [],
   "source": [
    "#The links to try\n",
    "original link = https://www.ndtv.com/india-news/details-of-dissent-letter-to-sonia-gandhi-steady-decline-no-honest-inspection-2286399\n",
    "l1= https://www.indiatoday.in/sports/ipl-2020/story/virat-kohli-rcb-dressing-room-speech-video-proud-of-ipl-campaign-fans-expectations-1738811-2020-11-07\n",
    "l2= https://sports.ndtv.com/ipl-2020/srh-vs-rcb-indian-premier-league-virat-kohli-shares-emotional-post-after-rcbs-ipl-2020-dream-ends-without-silverware-2321920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SucTw8VMOtay"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYB-cI2sOtdx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1mmPWm5OtW1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hx6Ro-XJOtPm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Adv NLP final test 0974.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
